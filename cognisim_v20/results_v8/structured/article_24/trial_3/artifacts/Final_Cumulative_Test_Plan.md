# Final Cumulative Test Plan

## Implementation Requirements

### Specific Execution Procedures and Tools

#### Execution Procedures
- Establish a Digital Operational Resilience (DOR) Committee and an Independent Testing Unit (ITU) with clear mandates for overseeing and conducting unbiased testing.
- Develop a DOR Testing Policy detailing the scope, approach, and methodologies for resilience testing, ensuring no conflicts of interest.
- Identify critical ICT systems and assets through a comprehensive risk assessment process involving stakeholders from all departments.
- Design test scenarios based on identified risks, using real-world examples and templates, including penetration testing, scenario analysis, and business continuity exercises.
- Schedule and conduct tests, including third-party involvement where necessary, during off-peak hours with contingency plans for rescheduling.
- Establish an Issue Management Framework (IMF) and a Testing Operations Team (TOT) for issue resolution.

#### Tools
- Risk assessment software (e.g., RSA Archer, MetricStream).
- Penetration testing tools (e.g., Metasploit, Nessus).
- Business continuity management software (e.g., Datto, Zerto).
- Incident management systems (e.g., ServiceNow, JIRA).
- Static code analysis tools (e.g., SonarQube) and dynamic analysis tools (e.g., OWASP ZAP).
- Performance testing tools (e.g., LoadRunner, Apache JMeter).
- Centralized dashboard and documentation repository (e.g., Tableau, Confluence).

#### Assessment Tools
- Utilize industry-standard cybersecurity frameworks such as NIST CSF, ISO 27001, and COBIT for comprehensive risk assessments.
- Deploy automated vulnerability scanning tools (e.g., Nessus, Qualys) and penetration testing software (e.g., Metasploit, Burp Suite).

#### Methodologies
- Adopt the MITRE ATT&CK framework for understanding and simulating adversary tactics and techniques.

#### Practices
- Implement a continuous integration/continuous deployment (CI/CD) pipeline for security testing in the software development lifecycle, including SAST and DAST tools.

### Testing Frequencies and Schedules
- Critical systems: Quarterly penetration testing, bi-annual business continuity testing.
- Non-critical systems: Bi-annual penetration testing, annual business continuity testing.
- Vulnerability Scanning: Conduct automated scans monthly and after any significant change in the network.
- Penetration Testing: Perform at least bi-annually and after major system updates or new system implementations.
- Risk Assessments: Execute semi-annually and in response to significant changes in the threat landscape or business operations.
- Schedule annual full-scale independent testing exercises and comprehensive risk assessments to align with strategic planning cycles.

### Resource Allocation and Role Definitions
- DOR Committee: Senior management from IT, Risk, Compliance, and Business Units.
- DOR Testing Team: IT security analysts, risk managers, business continuity planners.
- External Consultants: For independent testing and validation.
- Cybersecurity Team: Allocate a dedicated team responsible for implementing and managing the testing programme.
- Roles: Define roles such as Security Analysts, Penetration Testers, Compliance Officers, and IT Auditors.
- Training: Provide regular training to ensure the team is up-to-date with the latest tools and methodologies.
- Form dedicated teams: Issue Resolution Team (IRT) within the ITU and Testing Operations Team (TOT).
- Create a RACI matrix to clarify roles and responsibilities, including a Compliance Liaison within the TOT.

### Measurable KPIs and Success Criteria
- Number of tests conducted vs. planned.
- Number of identified vulnerabilities and their severity.
- Time to remediate critical vulnerabilities.
- Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO) adherence.
- Coverage: Percentage of critical systems covered by the testing programme.
- Remediation Time: Average time to remediate identified vulnerabilities.
- Test Frequency Compliance: Adherence to the predefined testing schedules.
- Incident Response: Time to detect and respond to simulated attacks during testing.
- Establish KPIs such as Mean Time to Detect (MTTD), Mean Time to Respond (MTTR), the number of critical vulnerabilities closed, and the time taken to close findings.
- Track the average time to resolve critical issues, aiming for a 15% reduction within the first year.
- Measure stakeholder satisfaction through surveys, aiming for an 85% satisfaction rate.
- Set a target for a 95% adherence rate to the issue resolution SLAs.

### Documentation and Audit Trail Processes
- Maintain a DOR Testing Log with details of each test, findings, and corrective actions.
- Use centralized repositories (e.g., SharePoint, Confluence) for documentation, with version control managed through Git.
- Implement version control and access management for sensitive documents.
- Test Reports: Document all findings from assessments and tests, including date, affected systems, risk level, and remediation actions.
- Change Management Logs: Maintain logs of all changes to the testing programme and rationale for changes.
- Audit Trails: Implement logging and monitoring tools to create an immutable record of all testing activities.
- Audit DOR documentation bi-annually to ensure accuracy and completeness.
- Develop comprehensive documentation for the IMF and testing procedures.
- Develop and enforce a documentation standard, including templates for test plans, reports, and audit trails.
- Implement a retention policy for issue-related records and audit trails, with a minimum duration of 5 years, and establish access controls based on role-based permissions.

## Quality Standards

### Immediately Actionable Guidance
- Assign the DOR Committee to initiate the DOR Testing Policy within 30 days.
- Conduct initial risk assessment within 60 days to identify critical systems.
- Complete the first round of testing within 180 days from policy initiation.
- Guidance Document: Create a detailed guidance document outlining the testing programme, including tools, schedules, and procedures.
- Training Sessions: Schedule mandatory training sessions for relevant staff on the new testing programme.
- Milestone deliverables include the formation of the DOR Committee, completion of the DOR Testing Policy, and integration of testing tools.
- Prioritize the development of the Independent Testing Procedure Manual, with a draft due by Q1 2025 and finalization by Q2 2025.
- Develop an IMF training program for relevant staff, with sessions starting by Q3 2025.
- Provide detailed guidelines for each testing phase, including pre-requisites, execution steps, and post-testing actions.

### Industry-Standard Tools and Vendors
- Select tools and vendors based on Gartnerâ€™s Magic Quadrant, Forrester Wave reports, user reviews, and case studies.
- Ensure vendors comply with ISO/IEC 27001 standards for information security management.
- Tools: Select tools with a strong industry reputation and support, ensuring they meet bank security requirements.
- Vendor Evaluation: Regularly review and evaluate vendors based on performance, support, and alignment with bank standards.
- Evaluate vendors annually, with the next evaluation scheduled for Q1 2026.
- Establish criteria for ad-hoc reviews in case of significant performance issues or changes in requirements.

### Concrete Timelines and Responsibilities
- DOR Committee formation: 30 days.
- DOR Testing Policy completion: 60 days.
- Initial risk assessment: 60 days.
- First round of testing: 180 days.
- Responsibilities are assigned as per the role definitions in the Resource Allocation section.
- Implementation Timeline: Establish a 6-month timeline for full implementation of the testing programme, with milestones for tool deployment, team training, and initial assessments.
- Responsibility Assignment: Assign clear responsibilities for each aspect of the programme to specific team members.
- Develop a risk mitigation plan for implementation delays by Q2 2025.
- Assign the IRT leader and a Project Manager to oversee the timely execution of the IMF and report progress to the DOR Committee monthly.

### Risk-Based Prioritization Frameworks
- Adopt the NIST Cybersecurity Framework and the FAIR model for risk assessment and prioritization.
- Use a quantitative risk assessment model to prioritize testing based on potential impact and likelihood.
- Prioritization Matrix: Develop a matrix to guide the frequency and depth of tests based on system classification (e.g., critical, high, medium, low).
- Update NIST and FAIR frameworks annually to reflect the evolving threat landscape.
- Utilize these frameworks to prioritize issues based on risk, ensuring that high-risk issues are addressed first.

### Compliance Mapping to Specific Article Sections
- Ensure the DOR Testing Programme aligns with the requirements of the Digital Operational Resilience Act (DORA) and relevant articles.
- Map testing activities to specific articles and maintain a compliance checklist.
- Article 25 Compliance: Ensure that the testing programme includes all elements required by Article 25, such as the range of tests and assessments.
- Article 26 Alignment: Align the testing methodologies and tools with the requirements specified in Article 26.
- Map DORA compliance to other regulations such as GDPR and PSD2.
- Create a comprehensive compliance map linking DORA requirements to the bank's policies and procedures.
- Establish a quarterly compliance checklist review process to incorporate regulatory changes and ensure ongoing compliance.

## Critical Instruction for Iterative Improvement

### Incorporating Previous Sprint's Feedback
- Set specific timelines for the formation of the ITU and the development of the DOR Testing Policy.
- Include stakeholders from various departments in the risk assessment process to ensure a comprehensive view.
- Provide test scenario templates to guide the ITU in designing effective test cases.
- Ensure tool integration and compatibility, with a focus on interoperability.
- Plan for ITU team training on the MITRE ATT&CK framework.
- Implement a RACI matrix and specify training content and frequency.
- Establish a feedback loop from stakeholders and conduct regular audits of documentation.
- Define deliverables for each milestone and make guidance documents widely available.
- Combine vendor selection sources and define criteria for regular vendor evaluations.
- Include a risk mitigation plan for implementation delays and ensure clear communication of responsibilities.
- Update risk-based prioritization frameworks annually and review the prioritization matrix bi-annually.
- Regularly review and update the compliance checklist to reflect regulatory changes.
- Define specific metrics that will trigger a review or update of the implementation plan and document the outcomes of sprint retrospectives, ensuring actionable items are tracked to completion.

### User Story: "Follow a Risk-Based Approach for Testing Programme" Implementation Plan
- Conduct an initial risk assessment to identify specific risks and criticality of information assets.
- Develop a risk-based testing program that prioritizes assets based on their criticality and the likelihood of threats.
- Exclude microenterprises from the risk assessment scope, as per the user story requirement.
- Regularly update the risk assessment to reflect the evolving ICT risk landscape.
- Implement the testing program, ensuring that it addresses the identified risks and adheres to the DOR Testing Policy.
- Review and refine the testing program annually to ensure it remains aligned with the bank's risk profile and organizational objectives.

## Additional Recommendations
- Review all timelines for feasibility, considering dependencies between tasks.
- Include a change management strategy to address resistance, with a plan developed by Q3 2025.
- Build flexibility into the test plan to adapt to regulatory changes or emerging threats, with a review and adaptation cycle established bi-annually.
- Ensure that the IMF is integrated with the bank's overall risk management framework for a holistic approach to issue management.
- Assess resource constraints and plan for skilled personnel recruitment or training as needed.
- Develop a change management strategy by Q3 2025 to address potential resistance and ensure smooth adoption of new processes and tools.
- Build flexibility into the test plan to adapt to external factors such as regulatory changes or emerging threats, with a semi-annual review process starting in Q3 2025.